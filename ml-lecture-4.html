<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks: A Complete Deep Dive</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Georgia, serif;
            background: linear-gradient(135deg, #FEF7ED 0%, #1E293B 100%);
            min-height: 100vh;
            color: #1E293B;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Base slide styling */
        .slide {
            background: #FEF7ED;
            margin-bottom: 30px;
            padding: 40px;
            border-radius: 15px;
            border-left: 5px solid #EA580C;
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
            min-height: 500px;
            page-break-after: always;
        }

        .slide:hover {
            transform: translateY(-5px);
        }

        /* Layout 1: Title Slide */
        .title-slide {
            background: #1E293B;
            color: #FEF7ED;
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            border-left: 5px solid #EA580C;
        }

        .title-slide h1 {
            font-size: 3.5em;
            margin-bottom: 30px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .title-slide .subtitle {
            font-size: 1.6em;
            opacity: 0.9;
            font-style: italic;
            margin-bottom: 20px;
        }

        .title-slide .author {
            font-size: 1.2em;
            opacity: 0.8;
        }

        /* Layout 2: Title and Content */
        .title-content h2 {
            font-size: 2.2em;
            color: #1E293B;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 3px solid #EA580C;
        }

        .content-area {
            font-size: 1.1em;
            line-height: 1.8;
        }

        .content-list {
            list-style: none;
            padding-left: 0;
        }

        .content-list li {
            position: relative;
            padding-left: 30px;
            margin-bottom: 15px;
        }

        .content-list li::before {
            content: "‚Üí";
            position: absolute;
            left: 0;
            color: #EA580C;
            font-weight: bold;
            font-size: 1.2em;
        }

        /* Layout 3: Section Header */
        .section-header {
            background: linear-gradient(135deg, #059669, #EA580C);
            color: #FEF7ED;
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }

        .section-header h1 {
            font-size: 3em;
            margin-bottom: 20px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .section-header .section-subtitle {
            font-size: 1.4em;
            opacity: 0.9;
        }

        /* Layout 4: Two Content */
        .two-content {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            align-items: start;
        }

        .two-content h2 {
            grid-column: 1 / -1;
            font-size: 2.2em;
            color: #1E293B;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 3px solid #EA580C;
        }

        .content-column {
            padding: 20px;
            background: rgba(5, 150, 105, 0.05);
            border-radius: 10px;
        }

        .content-column h3 {
            font-size: 1.6em;
            color: #059669;
            margin-bottom: 15px;
        }

        /* Layout 5: Comparison */
        .comparison h2 {
            font-size: 2.2em;
            color: #1E293B;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 3px solid #EA580C;
            text-align: center;
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin-top: 40px;
        }

        .comparison-item {
            text-align: center;
            padding: 30px;
            border-radius: 15px;
            background: rgba(234, 88, 12, 0.1);
        }

        .comparison-item h3 {
            font-size: 1.8em;
            color: #EA580C;
            margin-bottom: 20px;
        }

        .vs-divider {
            position: absolute;
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%);
            background: #EA580C;
            color: #FEF7ED;
            padding: 10px 20px;
            border-radius: 50px;
            font-weight: bold;
            font-size: 1.2em;
        }

        .comparison {
            position: relative;
        }

        /* Layout 6: Title Only */
        .title-only {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
        }

        .title-only h1 {
            font-size: 3em;
            color: #1E293B;
            margin-bottom: 20px;
        }

        .title-only .subtitle {
            font-size: 1.4em;
            color: #059669;
            font-style: italic;
        }

        /* Layout 7: Blank */
        .blank-slide {
            /* Intentionally minimal - just the base slide styling */
        }

        /* Layout 8: Content with Caption */
        .content-caption {
            display: grid;
            grid-template-rows: auto 1fr auto;
            gap: 30px;
        }

        .content-caption h2 {
            font-size: 2.2em;
            color: #1E293B;
            padding-bottom: 10px;
            border-bottom: 3px solid #EA580C;
        }

        .main-content-area {
            background: linear-gradient(45deg, #059669, #EA580C);
            border: 2px dashed #1E293B;
            border-radius: 15px;
            padding: 80px 40px;
            text-align: center;
            color: #FEF7ED;
            font-weight: bold;
            font-size: 1.2em;
        }

        .caption-area {
            background: rgba(5, 150, 105, 0.1);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #059669;
        }

        /* Layout 9: Picture with Caption */
        .picture-caption {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 40px;
            align-items: start;
        }

        .picture-caption h2 {
            grid-column: 1 / -1;
            font-size: 2.2em;
            color: #1E293B;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #EA580C;
        }

        .picture-area {
            background: rgba(5, 150, 105, 0.05);
            border: 2px solid #059669;
            border-radius: 15px;
            padding: 20px;
            text-align: center;
            min-height: 300px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .picture-area img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }

        .caption-sidebar {
            background: rgba(234, 88, 12, 0.1);
            padding: 30px;
            border-radius: 15px;
            border-left: 4px solid #EA580C;
        }

        .caption-sidebar h3 {
            color: #EA580C;
            margin-bottom: 15px;
            font-size: 1.4em;
        }

        /* Special Elements */
        .highlight-box {
            background: #059669;
            color: #FEF7ED;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: bold;
        }

        .accent-box {
            background: #EA580C;
            color: #FEF7ED;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-style: italic;
        }

        .emphasis {
            background: #EA580C;
            color: #FEF7ED;
            padding: 4px 8px;
            border-radius: 5px;
            font-weight: bold;
            display: inline-block;
        }

        .math-equation {
            background: #f8f9fa;
            border: 2px solid #059669;
            padding: 20px;
            margin: 20px 0;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 1.2em;
            text-align: center;
        }

        .key-insight {
            background: linear-gradient(135deg, #059669, #047857);
            color: #FEF7ED;
            padding: 25px;
            border-radius: 15px;
            margin: 25px 0;
            border-left: 5px solid #EA580C;
        }

        .key-insight h4 {
            margin-bottom: 10px;
            font-size: 1.3em;
        }

        .warning-box {
            background: #fef3c7;
            border: 2px solid #f59e0b;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .interactive-element {
            background: #e0e7ff;
            border: 2px solid #6366f1;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }

            .slide {
                padding: 20px;
                min-height: 400px;
            }

            .title-slide h1 {
                font-size: 2.5em;
            }

            .two-content,
            .comparison-grid,
            .picture-caption {
                grid-template-columns: 1fr;
                gap: 20px;
            }

            .vs-divider {
                display: none;
            }
        }

        /* Navigation */
        .slide-counter {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: #1E293B;
            color: #FEF7ED;
            padding: 10px 15px;
            border-radius: 25px;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="container">
        
        <!-- Title Slide -->
        <div class="slide title-slide">
            <h1>Neural Networks</h1>
            <p class="subtitle">A Complete Deep Dive: From Biological Inspiration to Mathematical Foundation</p>
            <p class="author">Chapter 1: But What IS a Neural Network? ‚Ä¢ Comprehensive University Course</p>
        </div>

        <!-- Course Overview -->
        <div class="slide title-content">
            <h2>üéØ Complete Learning Journey</h2>
            <div class="content-area">
                <p>This comprehensive course will transform you from a neural network novice to someone who truly understands these powerful systems from the ground up.</p>
                
                <div class="key-insight">
                    <h4>üß† Why This Matters</h4>
                    <p>Neural networks power everything from your smartphone's camera to autonomous vehicles. Understanding them isn't just academic‚Äîit's essential for anyone working in modern technology.</p>
                </div>
                
                <ul class="content-list">
                    <li><strong>Fundamental Challenge:</strong> Why traditional programming fails at pattern recognition</li>
                    <li><strong>Biological Inspiration:</strong> How brain-like structures solve impossible problems</li>
                    <li><strong>Mathematical Foundation:</strong> The elegant math that makes it all work</li>
                    <li><strong>Practical Implementation:</strong> From theory to working code</li>
                    <li><strong>Advanced Concepts:</strong> Weight optimization and learning algorithms</li>
                    <li><strong>Real-World Applications:</strong> How these concepts scale to modern AI</li>
                </ul>
            </div>
        </div>

        <!-- Section 1 Header -->
        <div class="slide section-header">
            <h1>Part I</h1>
            <p class="section-subtitle">The Fundamental Challenge</p>
        </div>

        <!-- The Paradox -->
        <div class="slide picture-caption">
            <h2>The Human-Computer Paradox</h2>
            
            <div class="picture-area">
                <img src="three-3s.png" alt="A cartoon œÄ character looks at three variations of the handwritten digit '3', highlighting the concept that humans can generalize different styles of the same digit, a task neural networks aim to mimic.">
            </div>
            
            <div class="caption-sidebar">
                <h3>ü§î The Paradox</h3>
                <p><strong>For Humans:</strong> Instant, effortless, automatic recognition across infinite variations.</p>
                
                <p><strong>For Computers:</strong> Each pixel must be analyzed, patterns must be hard-coded, exceptions must be manually programmed.</p>
                
                <div class="warning-box">
                    <strong>‚ö†Ô∏è Traditional Approach Fails</strong><br>
                    Try writing an if-statement to recognize ANY handwritten "3" - you'll quickly realize it's impossible!
                </div>
                
                <div class="accent-box">
                    This is exactly why we need a fundamentally different approach: Neural Networks.
                </div>
            </div>
        </div>

        <!-- Traditional vs Neural Comparison -->
        <div class="slide comparison">
            <h2>Traditional Programming vs Neural Networks</h2>
            
            <div class="comparison-grid">
                <div class="comparison-item">
                    <h3>Traditional Programming</h3>
                    <p><strong>Rule-Based Approach:</strong></p>
                    <ul class="content-list">
                        <li>Programmer writes explicit rules</li>
                        <li>If-then-else logic chains</li>
                        <li>Every scenario must be anticipated</li>
                        <li>Breaks down with complexity</li>
                        <li>Cannot handle exceptions gracefully</li>
                    </ul>
                    
                    <div class="math-equation">
                        if (pixel[100] > 0.5 && pixel[101] > 0.5) {<br>
                        &nbsp;&nbsp;// What goes here for "3"?<br>
                        }
                    </div>
                </div>
                
                <div class="comparison-item">
                    <h3>Neural Networks</h3>
                    <p><strong>Learning-Based Approach:</strong></p>
                    <ul class="content-list">
                        <li>Network learns patterns from examples</li>
                        <li>Automatic feature extraction</li>
                        <li>Handles unseen variations</li>
                        <li>Scales with data complexity</li>
                        <li>Graceful degradation</li>
                    </ul>
                    
                    <div class="math-equation">
                        Show 1000s of examples:<br>
                        "This is a 3", "This is a 3"...<br>
                        Network learns to recognize ANY 3!
                    </div>
                </div>
            </div>
            
            <div class="vs-divider">VS</div>
        </div>

        <!-- Section 2 Header -->
        <div class="slide section-header">
            <h1>Part II</h1>
            <p class="section-subtitle">Understanding Neurons: The Building Blocks</p>
        </div>

        <!-- What is a Neuron -->
        <div class="slide title-content">
            <h2>üß† Demystifying the "Neuron"</h2>
            <div class="content-area">
                <p>Let's strip away the biological metaphor and understand what a neural network neuron <em>actually</em> is:</p>
                
                <div class="highlight-box">
                    <strong>A neuron is simply:</strong> A container that holds a single number between 0.0 and 1.0
                </div>
                
                <p><strong>That's it.</strong> No complex biology, no mysterious processes. Just a number.</p>
                
                <div class="key-insight">
                    <h4>üî¢ The "Activation" Concept</h4>
                    <p>This number is called the neuron's <span class="emphasis">activation</span>:</p>
                    <ul class="content-list">
                        <li><strong>0.0:</strong> Neuron is "off" or inactive</li>
                        <li><strong>1.0:</strong> Neuron is "fully activated" or "fired up"</li>
                        <li><strong>0.3, 0.7, etc.:</strong> Partial activation levels</li>
                    </ul>
                </div>
                
                <div class="interactive-element">
                    <strong>üéØ Mental Model:</strong> Think of neurons like dimmer switches for lights. Each can be off (0), fully on (1), or anywhere in between. The pattern of all these "light levels" across the network represents information.
                </div>
            </div>
        </div>

        <!-- Input Layer Deep Dive -->
        <div class="slide picture-caption">
            <h2>Input Layer: Converting Reality to Numbers</h2>
            
            <div class="picture-area">
                <img src="pixel-values.png" alt="Zoomed-in view of a grayscale image (digit '3') showing pixel values from 0.0 to 1.0, representing normalized intensity levels used as input for a neural network.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üìä The Conversion Process</h3>
                <p><strong>Image ‚Üí Numbers:</strong></p>
                <ul class="content-list">
                    <li>28√ó28 pixel image = 784 total pixels</li>
                    <li>Each pixel: brightness value 0.0-1.0</li>
                    <li>Black pixels = 0.0</li>
                    <li>White pixels = 1.0</li>
                    <li>Gray pixels = values between</li>
                </ul>
                
                <div class="math-equation">
                    28 √ó 28 = 784 input neurons
                </div>
                
                <div class="key-insight">
                    <h4>üîÑ Information Encoding</h4>
                    <p>Every piece of information must be converted to numbers between 0 and 1 to be processed by a neural network.</p>
                </div>
            </div>
        </div>

        <!-- Input Layer Visualization -->
        <div class="slide picture-caption">
            <h2>The Input Layer in Action</h2>
            
            <div class="picture-area">
                <img src="highlight-first-layer.png" alt="Same digit classification network with the first input layer highlighted in yellow, emphasizing the 784 input nodes corresponding to 28x28 image pixels.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üîç Layer Breakdown</h3>
                <p><strong>Input Layer Structure:</strong></p>
                <ul class="content-list">
                    <li>784 neurons total</li>
                    <li>Each neuron = one pixel</li>
                    <li>Organized in conceptual 28√ó28 grid</li>
                    <li>Values fed simultaneously</li>
                </ul>
                
                <div class="warning-box">
                    <strong>Critical Point:</strong> The network doesn't see "images" - it only sees 784 numbers! The spatial relationship must be learned.
                </div>
                
                <div class="highlight-box">
                    <strong>Universal Principle:</strong> ANY input (text, audio, video) must be converted to numbers in this range.
                </div>
            </div>
        </div>

        <!-- Output Layer Deep Dive -->
        <div class="slide picture-caption">
            <h2>Output Layer: Making Predictions</h2>
            
            <div class="picture-area">
                <img src="output-layer.png" alt="A feedforward neural network diagram for digit recognition using the MNIST dataset. The input is a handwritten digit '9', converted into 784 input neurons, followed by multiple hidden layers, and finally a 10-neuron output layer. The highlighted output neuron (index 9) indicates the model's prediction.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üéØ Prediction Mechanism</h3>
                <p><strong>10 Output Neurons:</strong></p>
                <ul class="content-list">
                    <li>Neuron 0: "How much does this look like 0?"</li>
                    <li>Neuron 1: "How much does this look like 1?"</li>
                    <li>...</li>
                    <li>Neuron 9: "How much does this look like 9?"</li>
                </ul>
                
                <div class="math-equation">
                    Highest activation = Network's "best guess"
                </div>
                
                <div class="accent-box">
                    The beauty: Network can express uncertainty! Multiple neurons can be partially activated.
                </div>
            </div>
        </div>

        <!-- Uncertainty Example -->
        <div class="slide picture-caption">
            <h2>When Networks Are Uncertain</h2>
            
            <div class="picture-area">
                <img src="confused-output-question.png" alt="Digit classification neural network where multiple output nodes are activated with varying intensities. The output for '4' and '9' are darker, indicating uncertainty.">
            </div>
            
            <div class="caption-sidebar">
                <h3>ü§î Interpreting Uncertainty</h3>
                <p><strong>What This Tells Us:</strong></p>
                <ul class="content-list">
                    <li>Network is torn between "4" and "9"</li>
                    <li>Both neurons highly activated</li>
                    <li>Other digits have low confidence</li>
                    <li>This reflects realistic ambiguity!</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üß† Human-Like Reasoning</h4>
                    <p>Just like humans might be unsure between similar-looking digits, networks can express this uncertainty through activation patterns.</p>
                </div>
                
                <div class="interactive-element">
                    <strong>üéØ Think About It:</strong> How would you decide if a handwritten digit is a 4 or a 9? The network faces the same challenge!
                </div>
            </div>
        </div>

        <!-- Section 3 Header -->
        <div class="slide section-header">
            <h1>Part III</h1>
            <p class="section-subtitle">The Hidden Layers: Where Magic Happens</p>
        </div>

        <!-- Hidden Layers Mystery -->
        <div class="slide picture-caption">
            <h2>The Great Mystery: Hidden Layers</h2>
            
            <div class="picture-area">
                <img src="hidden-layers.png" alt="A neural network diagram with the hidden layers highlighted using a yellow box and question mark, symbolizing their mysterious role in feature learning.">
            </div>
            
            <div class="caption-sidebar">
                <h3>‚ùì The Big Questions</h3>
                <ul class="content-list">
                    <li>What are these layers actually doing?</li>
                    <li>How do they transform 784 numbers into 10 predictions?</li>
                    <li>Why not connect input directly to output?</li>
                </ul>
                
                <div class="warning-box">
                    <strong>The Challenge:</strong> We have 784 inputs, 10 outputs, and need something powerful enough to recognize any digit pattern.
                </div>
                
                <div class="key-insight">
                    <h4>üîç The Hypothesis</h4>
                    <p>Hidden layers learn to detect increasingly complex features, building up from simple to sophisticated patterns.</p>
                </div>
            </div>
        </div>

        <!-- Hierarchical Learning Theory -->
        <div class="slide two-content">
            <h2>The Hierarchical Learning Hypothesis</h2>
            
            <div class="content-column">
                <h3>üèóÔ∏è Layer 1: Edge Detection</h3>
                <p>The first hidden layer learns to detect basic edges and simple patterns.</p>
                
                <div class="math-equation">
                    Pixels ‚Üí Simple Edges
                </div>
                
                <ul class="content-list">
                    <li>Horizontal lines</li>
                    <li>Vertical lines</li>
                    <li>Diagonal edges</li>
                    <li>Simple curves</li>
                </ul>
                
                <div class="highlight-box">
                    <strong>Think:</strong> Like finding individual Lego pieces in a complex structure.
                </div>
            </div>
            
            <div class="content-column">
                <h3>üß© Layer 2: Pattern Assembly</h3>
                <p>The second hidden layer combines edges into meaningful patterns.</p>
                
                <div class="math-equation">
                    Simple Edges ‚Üí Complex Patterns
                </div>
                
                <ul class="content-list">
                    <li>Loops (for 0, 6, 8, 9)</li>
                    <li>Lines (for 1, 4, 7)</li>
                    <li>Curves (for 2, 3, 5)</li>
                    <li>Intersections</li>
                </ul>
                
                <div name="highlight-box">
                    <strong>Think:</strong> Like recognizing common shapes made from those Lego pieces.
                </div>
            </div>
        </div>

        <!-- Edge Detection Examples -->
        <div class="slide picture-caption">
            <h2>Layer 1: Edge Detection in Action</h2>
            
            <div class="picture-area">
                <img src="loop-edges.png" alt="Image decomposition of a handwritten digit '0' into several edge components: diagonal, curve, and horizontal segments. Each colored box shows a different learned feature from the image.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üîç Breaking Down Complexity</h3>
                <p><strong>Digit "0" Analysis:</strong></p>
                <ul class="content-list">
                    <li>Top curve (red box)</li>
                    <li>Right edge (blue box)</li>
                    <li>Bottom curve (green box)</li>
                    <li>Left edge (yellow box)</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üéØ The Insight</h4>
                    <p>Complex shapes are combinations of simpler edges. If we can detect edges, we can detect shapes!</p>
                </div>
                
                <div class="math-equation">
                    4 edges detected ‚Üí Likely a "0"
                </div>
            </div>
        </div>

        <div class="slide picture-caption">
            <h2>More Edge Detection Examples</h2>
            
            <div class="picture-area">
                <img src="line-edges.png" alt="Image decomposition of a digit '1' into vertical and slightly slanted edges using learned features. Shows how a digit is built from simpler components.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üîç Digit "1" Analysis</h3>
                <p><strong>Simpler Pattern:</strong></p>
                <ul class="content-list">
                    <li>Main vertical line (blue)</li>
                    <li>Top angle (red)</li>
                    <li>Base extension (yellow)</li>
                </ul>
                
                <div class="interactive-element">
                    <strong>ü§î Notice:</strong> Different digits require different types of edge detection. The network must learn to detect ALL possible edge types!
                </div>
                
                <div class="accent-box">
                    <strong>Scaling Up:</strong> Imagine doing this for every possible handwriting style, for every digit, automatically!
                </div>
            </div>
        </div>

        <!-- Universal Pattern Recognition -->
        <div class="slide picture-caption">
            <h2>Beyond Digits: Universal Pattern Recognition</h2>
            
            <div class="picture-area">
                <img src="edge-detection.png" alt="Side-by-side image showing a lion on the left and its corresponding edge-detected version on the right, used to explain how neural networks or vision systems detect edges.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üåç Universal Applications</h3>
                <p><strong>Edge Detection Works For:</strong></p>
                <ul class="content-list">
                    <li>Animal recognition</li>
                    <li>Face detection</li>
                    <li>Medical imaging</li>
                    <li>Satellite imagery</li>
                    <li>Quality control</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üöÄ Powerful Principle</h4>
                    <p>The same hierarchical approach works across completely different domains!</p>
                </div>
                
                <div class="highlight-box">
                    <strong>This is why neural networks are revolutionary:</strong> One approach, infinite applications.
                </div>
            </div>
        </div>

        <!-- Audio Example -->
        <div class="slide picture-caption">
            <h2>Hierarchical Processing in Other Domains</h2>
            
            <div class="picture-area">
                <img src="audio.png" alt="A visual of raw audio waveform transforming into the word 'recognition' through successive processing stages: audio ‚ûù text ‚ûù syllables ‚ûù final cleaned-up text. Demonstrates the transformation in speech recognition.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üéµ Speech Recognition Hierarchy</h3>
                <p><strong>Layer by Layer:</strong></p>
                <ul class="content-list">
                    <li><strong>Layer 1:</strong> Raw audio ‚Üí Basic sounds</li>
                    <li><strong>Layer 2:</strong> Sounds ‚Üí Phonemes</li>
                    <li><strong>Layer 3:</strong> Phonemes ‚Üí Syllables</li>
                    <li><strong>Layer 4:</strong> Syllables ‚Üí Words</li>
                    <li><strong>Layer 5:</strong> Words ‚Üí Meaning</li>
                </ul>
                
                <div class="math-equation">
                    Complex Problem = Hierarchy of Simple Steps
                </div>
                
                <div class="accent-box">
                    <strong>Universal Truth:</strong> Most intelligent tasks can be broken down into hierarchical processing steps.
                </div>
            </div>
        </div>

        <!-- Section 4 Header -->
        <div class="slide section-header">
            <h1>Part IV</h1>
            <p class="section-subtitle">The Mathematics: How Information Flows</p>
        </div>

        <!-- Weights Introduction -->
        <div class="slide title-content">
            <h2>üîó Understanding Weights: The Connection Strength</h2>
            <div class="content-area">
                <p>Now we get to the heart of how neural networks actually work. The magic lies in the <span class="emphasis">weights</span>.</p>
                
                <div class="key-insight">
                    <h4>üß† The Core Concept</h4>
                    <p>A <strong>weight</strong> is simply a number that determines how much influence one neuron has on another. That's it.</p>
                </div>
                
                <p><strong>Here's how it works:</strong></p>
                <ul class="content-list">
                    <li><strong>Positive weight:</strong> "When the first neuron is active, the second should be active too"</li>
                    <li><strong>Negative weight:</strong> "When the first neuron is active, the second should be inactive"</li>
                    <li><strong>Zero weight:</strong> "These neurons don't influence each other"</li>
                    <li><strong>Large magnitude:</strong> Strong influence (positive or negative)</li>
                    <li><strong>Small magnitude:</strong> Weak influence</li>
                </ul>
                
                <div class="interactive-element">
                    <strong>üéØ Analogy:</strong> Think of weights like volume controls. Some connections are turned up loud (high positive/negative), others are barely audible (near zero).
                </div>
            </div>
        </div>

        <!-- Specific Edge Detection Example -->
        <div class="slide picture-caption">
            <h2>Weights in Action: Detecting a Specific Edge</h2>
            
            <div class="picture-area">
                <img src="desired-edge.png" alt="A neuron connected to a 784 input grid with a small white rectangle in the image. Demonstrates how a single neuron might detect a specific localized feature (like an edge) in an image.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üéØ The Goal</h3>
                <p><strong>We want this neuron to detect this specific edge pattern.</strong></p>
                
                <p><strong>Strategy:</strong></p>
                <ul class="content-list">
                    <li>High positive weights where we want bright pixels</li>
                    <li>High negative weights where we want dark pixels</li>
                    <li>Zero weights where we don't care</li>
                </ul>
                
                <div class="math-equation">
                    Response = Œ£(weight √ó pixel_value)
                </div>
                
                <div class="key-insight">
                    <h4>üí° The Insight</h4>
                    <p>By carefully choosing 784 weights, we can make this neuron respond strongly to our desired pattern and weakly to others!</p>
                </div>
            </div>
        </div>

        <!-- Weight Visualization -->
        <div class="slide picture-caption">
            <h2>Visualizing Weights: The Complete Picture</h2>
            
            <div class="picture-area">
                <img src="13002-blue.png" alt="Fully connected neural network with weights shown in blue and red colors based on polarity. Side box breaks down the total number of weights and biases‚Äî13,002 in total.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üìä The Numbers</h3>
                <p><strong>Weight Breakdown:</strong></p>
                <ul class="content-list">
                    <li>Input to Hidden 1: 784√ó16 = 12,544 weights</li>
                    <li>Hidden 1 to Hidden 2: 16√ó16 = 256 weights</li>
                    <li>Hidden 2 to Output: 16√ó10 = 160 weights</li>
                    <li>Plus biases: 16+16+10 = 42</li>
                </ul>
                
                <div class="math-equation">
                    Total: 13,002 parameters to learn!
                </div>
                
                <div class="warning-box">
                    <strong>‚ö†Ô∏è Complexity Alert:</strong> Each of these 13,002 numbers must be precisely tuned for the network to work!
                </div>
            </div>
        </div>

        <!-- Weighted Sum Calculation -->
        <div class="slide title-content">
            <h2>üßÆ The Weighted Sum: Core Calculation</h2>
            <div class="content-area">
                <p>Every neuron in a hidden or output layer performs the same fundamental calculation:</p>
                
                <div class="math-equation">
                    weighted_sum = w‚ÇÅ√óa‚ÇÅ + w‚ÇÇ√óa‚ÇÇ + w‚ÇÉ√óa‚ÇÉ + ... + w‚Çô√óa‚Çô
                </div>
                
                <p>Where:</p>
                <ul class="content-list">
                    <li><strong>w·µ¢</strong> = weight of connection i</li>
                    <li><strong>a·µ¢</strong> = activation of neuron i in previous layer</li>
                    <li><strong>n</strong> = number of neurons in previous layer</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üéØ What This Means</h4>
                    <p>Each neuron is asking: "Based on the pattern of activations in the previous layer, and given my weights, how excited should I be?"</p>
                </div>
                
                <div class="interactive-element">
                    <strong>ü§î Think About It:</strong> If a neuron has learned to detect horizontal lines, it will have positive weights for horizontally-aligned pixels and negative weights elsewhere.
                </div>
            </div>
        </div>

        <!-- Section 5 Header -->
        <div class="slide section-header">
            <h1>Part V</h1>
            <p class="section-subtitle">Activation Functions: The Squishification</p>
        </div>

        <!-- Why We Need Squishing -->
        <div class="slide picture-caption">
            <h2>The Problem: Unlimited Range</h2>
            
            <div class="picture-area">
                <img src="number-line-squish.png" alt="A number line with a yellow arrow pointing at the small region between 0 and 1, emphasizing that neural activations are squished into this range after applying an activation function like sigmoid.">
            </div>
            
            <div class="caption-sidebar">
                <h3>‚ö†Ô∏è The Challenge</h3>
                <p><strong>Weighted Sum Problems:</strong></p>
                <ul class="content-list">
                    <li>Can be any number: -1000, +500, etc.</li>
                    <li>Our neurons need values 0.0-1.0</li>
                    <li>Need smooth, differentiable function</li>
                    <li>Should preserve relative ordering</li>
                </ul>
                
                <div class="math-equation">
                    Need: ‚Ñù ‚Üí [0,1]
                </div>
                
                <div class="key-insight">
                    <h4>üéØ Requirements</h4>
                    <p>We need a function that smoothly maps any real number to our desired 0-1 range, while preserving the relative magnitudes.</p>
                </div>
            </div>
        </div>

        <!-- Sigmoid Function -->
        <div class="slide picture-caption">
            <h2>The Sigmoid Function: Perfect Solution</h2>
            
            <div class="picture-area">
                <img src="sigmoid.png" alt="A graph of the sigmoid activation function: œÉ(x) = 1/(1 + e^(-x)). The curve transitions smoothly from 0 to 1 as x increases.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üìà Sigmoid Properties</h3>
                <p><strong>Mathematical Definition:</strong></p>
                
                <div class="math-equation">
                    œÉ(x) = 1/(1 + e^(-x))
                </div>
                
                <p><strong>Key Properties:</strong></p>
                <ul class="content-list">
                    <li>Range: (0, 1) - perfect for our needs!</li>
                    <li>Smooth and continuous</li>
                    <li>Differentiable everywhere</li>
                    <li>S-shaped curve</li>
                    <li>œÉ(0) = 0.5 (midpoint)</li>
                </ul>
                
                <div class="interactive-element">
                    <strong>üß™ Test Your Understanding:</strong> What is œÉ(-1000)? Close to 0! What about œÉ(1000)? Close to 1!
                </div>
            </div>
        </div>

        <!-- Bias Introduction -->
        <div class="slide picture-caption">
            <h2>Bias: Fine-Tuning Activation</h2>
            
            <div class="picture-area">
                <img src="bias.png" alt="Mathematical expression with a highlighted negative bias (-10) and a caption stating that neurons only activate meaningfully when the weighted sum exceeds 10. Explains the role of bias in neural nets.">
            </div>
            
            <div class="caption-sidebar">
                <h3>‚öôÔ∏è The Role of Bias</h3>
                <p><strong>Problem:</strong> What if we don't want the neuron to activate when weighted_sum > 0?</p>
                
                <p><strong>Solution:</strong> Add a bias term!</p>
                
                <div class="math-equation">
                    activation = œÉ(weighted_sum + bias)
                </div>
                
                <p><strong>Bias Effects:</strong></p>
                <ul class="content-list">
                    <li><strong>Negative bias:</strong> Harder to activate</li>
                    <li><strong>Positive bias:</strong> Easier to activate</li>
                    <li><strong>Zero bias:</strong> Activates at weighted_sum = 0</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üéØ Control Mechanism</h4>
                    <p>Bias lets us control exactly when each neuron should "fire"!</p>
                </div>
            </div>
        </div>

        <!-- Complete Neuron Formula -->
        <div class="slide title-content">
            <h2>üßÆ The Complete Neuron Formula</h2>
            <div class="content-area">
                <p>Now we can write the complete formula for any neuron's activation:</p>
                
                <div class="math-equation">
                    a‚±º = œÉ(w‚ÇÅ‚±ºa‚ÇÅ + w‚ÇÇ‚±ºa‚ÇÇ + w‚ÇÉ‚±ºa‚ÇÉ + ... + w‚Çô‚±ºa‚Çô + b‚±º)
                </div>
                
                <p><strong>Where:</strong></p>
                <ul class="content-list">
                    <li><strong>a‚±º</strong> = activation of neuron j in current layer</li>
                    <li><strong>w·µ¢‚±º</strong> = weight from neuron i (prev layer) to neuron j (current layer)</li>
                    <li><strong>a·µ¢</strong> = activation of neuron i in previous layer</li>
                    <li><strong>b‚±º</strong> = bias of neuron j</li>
                    <li><strong>œÉ</strong> = sigmoid function</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üéâ This is it!</h4>
                    <p>This single formula describes how every neuron in every hidden and output layer computes its activation. The entire network is just this formula applied thousands of times!</p>
                </div>
                
                <div class="highlight-box">
                    <strong>Universal Truth:</strong> Despite all the complexity we've discussed, every neuron does exactly the same simple calculation!
                </div>
            </div>
        </div>

        <!-- Section 6 Header -->
        <div class="slide section-header">
            <h1>Part VI</h1>
            <p class="section-subtitle">Matrix Mathematics: Elegant Notation</p>
        </div>

        <!-- Matrix Representation -->
        <div class="slide picture-caption">
            <h2>Matrix Magic: Computing All Neurons at Once</h2>
            
            <div class="picture-area">
                <img src="annotated-equation.png" alt="Annotated equation showing how activations are calculated in a neural network layer. Highlights how superscripts represent layers, subscripts represent neurons, and biases are added after weighted sums.">
            </div>
            
            <div class="caption-sidebar">
                <h3>üßÆ Matrix Power</h3>
                <p><strong>Instead of computing each neuron individually, we can compute ALL neurons in a layer simultaneously using matrix operations!</strong></p>
                
                <div class="math-equation">
                    a‚ÅΩÀ°‚Å∫¬π‚Åæ = œÉ(W‚ÅΩÀ°‚Åæa‚ÅΩÀ°‚Åæ + b‚ÅΩÀ°‚Åæ)
                </div>
                
                <p><strong>Notation Guide:</strong></p>
                <ul class="content-list">
                    <li><strong>a‚ÅΩÀ°‚Åæ:</strong> Activations of layer l</li>
                    <li><strong>W‚ÅΩÀ°‚Åæ:</strong> Weight matrix from layer l to l+1</li>
                    <li><strong>b‚ÅΩÀ°‚Åæ:</strong> Bias vector for layer l+1</li>
                    <li><strong>œÉ:</strong> Applied element-wise</li>
                </ul>
            </div>
        </div>

        <!-- Matrix Details -->
        <div class="slide title-content">
            <h2>üìê Matrix Dimensions: Getting the Math Right</h2>
            <div class="content-area">
                <p>Understanding matrix dimensions is crucial for implementing neural networks:</p>
                
                <div class="key-insight">
                    <h4>üìä Dimension Analysis</h4>
                    <p><strong>For a layer with n input neurons and m output neurons:</strong></p>
                    <ul class="content-list">
                        <li><strong>Weight matrix W:</strong> m √ó n (rows = outputs, cols = inputs)</li>
                        <li><strong>Input activations a:</strong> n √ó 1 (column vector)</li>
                        <li><strong>Bias vector b:</strong> m √ó 1 (column vector)</li>
                        <li><strong>Output activations:</strong> m √ó 1 (column vector)</li>
                    </ul>
                </div>
                
                <div class="math-equation">
                    (m √ó n) √ó (n √ó 1) + (m √ó 1) = (m √ó 1)
                </div>
                
                <p><strong>Why This Matters:</strong></p>
                <ul class="content-list">
                    <li>Enables vectorized computation (much faster!)</li>
                    <li>Libraries like NumPy/TensorFlow optimize matrix operations</li>
                    <li>Makes code cleaner and more readable</li>
                    <li>Essential for implementing backpropagation (next lesson!)</li>
                </ul>
                
                <div class="interactive-element">
                    <strong>üéØ Pro Tip:</strong> Always check your matrix dimensions when implementing neural networks. Dimension mismatches are the #1 source of bugs!
                </div>
            </div>
        </div>

        <!-- Section 7 Header -->
        <div class="slide section-header">
            <h1>Part VII</h1>
            <p class="section-subtitle">The Big Picture: Understanding the Complete System</p>
        </div>

        <!-- Network as Function -->
        <div class="slide title-content">
            <h2>üîß The Network as a Function</h2>
            <div class="content-area">
                <p>Let's step back and see the forest for the trees:</p>
                
                <div class="highlight-box">
                    <strong>Fundamental Truth:</strong> A neural network is just a very complicated function that takes numbers as input and produces numbers as output.
                </div>
                
                <p><strong>For our digit recognizer:</strong></p>
                <ul class="content-list">
                    <li><strong>Input:</strong> 784 numbers (pixel values)</li>
                    <li><strong>Output:</strong> 10 numbers (digit probabilities)</li>
                    <li><strong>Parameters:</strong> 13,002 weights and biases</li>
                    <li><strong>Operations:</strong> Matrix multiplications and sigmoid applications</li>
                </ul>
                
                <div class="math-equation">
                    f(pixel‚ÇÅ, pixel‚ÇÇ, ..., pixel‚Çá‚Çà‚ÇÑ) = (prob‚ÇÄ, prob‚ÇÅ, ..., prob‚Çâ)
                </div>
                
                <div class="key-insight">
                    <h4>ü§Ø Mind-Blowing Realization</h4>
                    <p>This "function" can recognize handwritten digits better than most humans, yet it's just arithmetic operations applied in sequence!</p>
                </div>
            </div>
        </div>

        <!-- Complexity and Beauty -->
        <div class="slide two-content">
            <h2>Complexity Meets Elegance</h2>
            
            <div class="content-column">
                <h3>üî• The Complexity</h3>
                <ul class="content-list">
                    <li>13,002 parameters to tune</li>
                    <li>Thousands of multiplication operations</li>
                    <li>Non-linear transformations at each layer</li>
                    <li>Intricate interaction patterns</li>
                    <li>Emergent intelligent behavior</li>
                </ul>
                
                <div class="warning-box">
                    <strong>Humbling Fact:</strong> No human could ever manually set these 13,002 parameters to make the network work!
                </div>
            </div>
            
            <div class="content-column">
                <h3>‚ú® The Elegance</h3>
                <ul class="content-list">
                    <li>Every neuron follows the same simple rule</li>
                    <li>Just weighted sums and sigmoid functions</li>
                    <li>Beautiful mathematical structure</li>
                    <li>Scalable to any size network</li>
                    <li>Universal approximation capability</li>
                </ul>
                
                <div class="highlight-box">
                    <strong>Beautiful Truth:</strong> Infinite complexity emerges from infinite repetition of simple operations!
                </div>
            </div>
        </div>

        <!-- What We Haven't Covered -->
        <div class="slide title-content">
            <h2>üîÆ The Mystery Remains: How Do We Learn?</h2>
            <div class="content-area">
                <p>We now understand the structure and mathematics, but the biggest question remains:</p>
                
                <div class="accent-box">
                    <strong>The $64,000 Question:</strong> How does the network learn the right values for those 13,002 parameters?
                </div>
                
                <p><strong>What we know so far:</strong></p>
                <ul class="content-list">
                    <li>‚úÖ What neurons are and how they work</li>
                    <li>‚úÖ How layers organize to solve complex problems</li>
                    <li>‚úÖ How weights and biases control behavior</li>
                    <li>‚úÖ How activation functions keep values in range</li>
                    <li>‚úÖ How matrix math makes it all efficient</li>
                </ul>
                
                <p><strong>What we still need to learn:</strong></p>
                <ul class="content-list">
                    <li>‚ùì How do we find the right weight values?</li>
                    <li>‚ùì What does "learning from examples" actually mean?</li>
                    <li>‚ùì How do we measure if our network is improving?</li>
                    <li>‚ùì How do we automatically adjust thousands of parameters?</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üéØ Next Chapter Preview</h4>
                    <p>The learning process involves <strong>gradient descent</strong> and <strong>backpropagation</strong> - elegant mathematical techniques that automatically adjust all 13,002 parameters to minimize prediction errors!</p>
                </div>
            </div>
        </div>

        <!-- Final Summary -->
        <div class="slide title-content">
            <h2>üéì Chapter 1 Complete: What You've Mastered</h2>
            <div class="content-area">
                <div class="highlight-box">
                    <strong>üèÜ Congratulations!</strong> You now have a deep, comprehensive understanding of neural network architecture and computation.
                </div>
                
                <p><strong>Key Concepts Mastered:</strong></p>
                <ul class="content-list">
                    <li><strong>The Fundamental Challenge:</strong> Why traditional programming fails at pattern recognition</li>
                    <li><strong>Neuron Mechanics:</strong> Simple containers holding numbers between 0 and 1</li>
                    <li><strong>Layer Architecture:</strong> Input ‚Üí Hidden ‚Üí Output structure and why it works</li>
                    <li><strong>Hierarchical Learning:</strong> Edges ‚Üí Patterns ‚Üí Recognition</li>
                    <li><strong>Weight Mathematics:</strong> How connections determine behavior</li>
                    <li><strong>Activation Functions:</strong> Sigmoid squishification and bias control</li>
                    <li><strong>Matrix Representation:</strong> Elegant mathematical notation</li>
                    <li><strong>System Perspective:</strong> Networks as complex functions</li>
                </ul>
                
                <div class="key-insight">
                    <h4>üöÄ You're Ready for Advanced Topics</h4>
                    <p>With this foundation, you can now tackle gradient descent, backpropagation, and modern neural network architectures with confidence!</p>
                </div>
                
                <div class="interactive-element">
                    <strong>üéØ Challenge Yourself:</strong> Can you explain to someone else how a neural network recognizes handwritten digits? If yes, you've truly mastered this material!
                </div>
            </div>
        </div>

    </div>

    <!-- Slide Counter -->
    <div class="slide-counter">
        Slide 1 of 30
    </div>

    <script>
        // Update slide counter on scroll
        function updateSlideCounter() {
            const slides = document.querySelectorAll('.slide');
            const slideCounter = document.querySelector('.slide-counter');
            
            slides.forEach((slide, index) => {
                const rect = slide.getBoundingClientRect();
                if (rect.top <= 100 && rect.bottom >= 100) {
                    slideCounter.textContent = `Slide ${index + 1} of ${slides.length}`;
                }
            });
        }

        window.addEventListener('scroll', updateSlideCounter);
        
        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            const slides = document.querySelectorAll('.slide');
            let currentSlide = 0;
            
            // Find current slide
            slides.forEach((slide, index) => {
                const rect = slide.getBoundingClientRect();
                if (rect.top <= 100 && rect.bottom >= 100) {
                    currentSlide = index;
                }
            });
            
            // Navigate with arrow keys
            if (e.key === 'ArrowDown' || e.key === 'ArrowRight') {
                if (currentSlide < slides.length - 1) {
                    slides[currentSlide + 1].scrollIntoView({ behavior: 'smooth' });
                }
            } else if (e.key === 'ArrowUp' || e.key === 'ArrowLeft') {
                if (currentSlide > 0) {
                    slides[currentSlide - 1].scrollIntoView({ behavior: 'smooth' });
                }
            }
        });
    </script>
</body>
</html>
